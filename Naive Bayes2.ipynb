{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68168a0-fcdc-47fd-8186-79a1fe02d484",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f24886-1e0c-4c57-8fd4-09cf84c86c31",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. Let's denote:\n",
    "\n",
    "- \\( S \\): the event that an employee is a smoker.\n",
    "- \\( H \\): the event that an employee uses the health insurance plan.\n",
    "\n",
    "We are given:\n",
    "\n",
    "\\[ P(H) = 0.70 \\] (the probability that an employee uses the health insurance plan)\n",
    "\n",
    "\\[ P(S|H) = 0.40 \\] (the probability that an employee is a smoker given that he/she uses the health insurance plan)\n",
    "\n",
    "Bayes' theorem is given by:\n",
    "\n",
    "\\[ P(S|H) = \\frac{P(H|S) \\cdot P(S)}{P(H)} \\]\n",
    "\n",
    "We can rearrange this formula to solve for \\( P(H|S) \\), which represents the probability that an employee uses the health insurance plan given that he/she is a smoker:\n",
    "\n",
    "\\[ P(H|S) = \\frac{P(S|H) \\cdot P(H)}{P(S)} \\]\n",
    "\n",
    "We are not directly given \\( P(S) \\), the probability that an employee is a smoker. However, we can find it using the law of total probability:\n",
    "\n",
    "\\[ P(S) = P(S|H) \\cdot P(H) + P(S|\\neg H) \\cdot P(\\neg H) \\]\n",
    "\n",
    "Here, \\( \\neg H \\) denotes the event that an employee does not use the health insurance plan, and \\( P(S|\\neg H) \\) is the probability that an employee is a smoker given that he/she does not use the health insurance plan.\n",
    "\n",
    "Now, let's calculate:\n",
    "\n",
    "\\[ P(S) = P(S|H) \\cdot P(H) + P(S|\\neg H) \\cdot P(\\neg H) \\]\n",
    "\n",
    "\\[ P(S) = 0.40 \\cdot 0.70 + P(S|\\neg H) \\cdot (1 - 0.70) \\]\n",
    "\n",
    "Since we don't have information about \\( P(S|\\neg H) \\) in the given information, we can't calculate the exact value of \\( P(S) \\). However, if we assume that the smoking rate is independent of whether an employee uses the health insurance plan (\\( P(S|\\neg H) = P(S) \\)), we can simplify the calculation:\n",
    "\n",
    "\\[ P(S) \\approx 0.40 \\cdot 0.70 + 0.40 \\cdot (1 - 0.70) \\]\n",
    "\n",
    "Now, let's plug this into Bayes' theorem to find \\( P(H|S) \\):\n",
    "\n",
    "\\[ P(H|S) = \\frac{P(S|H) \\cdot P(H)}{P(S)} \\]\n",
    "\n",
    "\\[ P(H|S) = \\frac{0.40 \\cdot 0.70}{0.40 \\cdot 0.70 + 0.40 \\cdot (1 - 0.70)} \\]\n",
    "\n",
    "You can now calculate this expression to find the probability that an employee uses the health insurance plan given that he/she is a smoker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c9001-1445-4897-a30b-389b5f9000d7",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96094a3a-6119-494a-8685-0a6eb4ca2558",
   "metadata": {},
   "source": [
    "The primary difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of features they are designed to work with and how they model the data.\n",
    "\n",
    "1. **Type of Features:**\n",
    "   - **Bernoulli Naive Bayes:** It is suitable for binary feature data, where each feature is a binary variable (e.g., presence or absence of a word in a document). It models the presence or absence of each feature and assumes that each feature is conditionally independent given the class.\n",
    "   - **Multinomial Naive Bayes:** It is designed for discrete feature data, typically representing counts or frequencies of events (e.g., word frequencies in a document). It works well with data that can be modeled as counts or occurrences of multiple categories and assumes that each feature is conditionally independent given the class.\n",
    "\n",
    "2. **Modeling Approach:**\n",
    "   - **Bernoulli Naive Bayes:** It models the presence or absence of each feature using binary values (0 or 1). It assumes that the features are conditionally independent given the class and calculates probabilities based on the frequency of feature occurrences.\n",
    "   - **Multinomial Naive Bayes:** It models the counts or frequencies of each feature in the document. It assumes a multinomial distribution for the data, and the probabilities are based on the observed frequencies of different features given the class.\n",
    "\n",
    "3. **Suitability:**\n",
    "   - **Bernoulli Naive Bayes:** It is often used in text classification tasks, where the presence or absence of certain words in a document is important.\n",
    "   - **Multinomial Naive Bayes:** It is commonly used in document classification tasks where the frequency of words is essential for determining the class.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the features in the dataset. If the features are binary (e.g., word presence or absence), Bernoulli Naive Bayes may be more appropriate. If the features are counts or frequencies of events, Multinomial Naive Bayes is a suitable choice. Each variant makes different assumptions about the underlying data distribution and is well-suited to specific types of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdaadd4-54d7-4d0f-833c-f9510ac31810",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61f64f-8a51-4e6e-8bbd-16240d46f184",
   "metadata": {},
   "source": [
    "In the context of Naive Bayes, including Bernoulli Naive Bayes, missing values can pose challenges. The handling of missing values depends on the specific implementation and the strategy adopted. Here are a few common approaches:\n",
    "\n",
    "1. **Ignoring Missing Values:**\n",
    "   - One simple approach is to ignore instances with missing values during both training and prediction. This means that any instance containing at least one missing value would be excluded from the analysis.\n",
    "\n",
    "2. **Imputation:**\n",
    "   - Another approach is to impute missing values with a specific value. For Bernoulli Naive Bayes, which often deals with binary features (presence or absence of a feature), you might choose to impute missing values with a default value (either 0 or 1, based on the context).\n",
    "\n",
    "3. **Accounting for Missing Values in Probabilities:**\n",
    "   - Some implementations of Naive Bayes allow you to explicitly account for missing values in the probability calculations. For example, when calculating the probability of a feature given a class, instances with missing values in that feature may be treated separately or assigned a specific probability.\n",
    "\n",
    "4. **Special Handling for Missing Values:**\n",
    "   - In some implementations, you might have the option to treat missing values as a separate category. This involves considering the missing values as a distinct state of the feature, and probabilities are calculated accordingly.\n",
    "\n",
    "5. **Advanced Imputation Techniques:**\n",
    "   - For more sophisticated scenarios, advanced imputation techniques such as k-nearest neighbors imputation or matrix factorization methods can be applied. However, the choice of imputation method depends on the specific characteristics of the data.\n",
    "\n",
    "It's essential to note that the choice of how to handle missing values depends on the characteristics of the dataset and the problem at hand. In the case of Bernoulli Naive Bayes, the binary nature of features adds some simplicity to the handling of missing values. Still, the specific strategy should align with the assumptions made about the data and the context in which the model is applied. Additionally, it's crucial to carefully evaluate the impact of the chosen approach on the performance of the model, as handling missing values can influence the overall predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e1481-1207-4913-b537-5dd3f58e4765",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef92fc3-7d1b-44a5-b138-b27aa33b71b8",
   "metadata": {},
   "source": [
    "In the case of Gaussian Naive Bayes, the algorithm assumes that the continuous-valued features for each class are normally distributed. The likelihoods for each class are modeled using the mean and standard deviation of the observed feature values. The decision rule involves choosing the class with the highest posterior probability.\n",
    "\n",
    "For multi-class classification, Gaussian Naive Bayes essentially extends the binary classification approach to multiple classes by comparing the posterior probabilities for each class and selecting the one with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b977f5-03ab-414f-a420-db09409c1786",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301797e1-2d28-40a0-b2ae-2d27d9866ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 Score: 0.8481249015095276\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 Score: 0.7282909724016348\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 Score: 0.8130660909542995\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Spambase dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "columns = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n",
    "    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n",
    "    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n",
    "    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\",\n",
    "    \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\",\n",
    "    \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\",\n",
    "    \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\",\n",
    "    \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\",\n",
    "    \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n",
    "    \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\",\n",
    "    \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\",\n",
    "    \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\",\n",
    "    \"capital_run_length_longest\", \"capital_run_length_total\", \"spam_label\"\n",
    "]\n",
    "data = pd.read_csv(url, header=None, names=columns)\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(\"spam_label\", axis=1)\n",
    "y = data[\"spam_label\"]\n",
    "\n",
    "# Define classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Evaluate classifiers using 10-fold cross-validation\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy = np.mean(cross_val_score(classifier, X, y, cv=10, scoring='accuracy'))\n",
    "    precision = np.mean(cross_val_score(classifier, X, y, cv=10, scoring='precision'))\n",
    "    recall = np.mean(cross_val_score(classifier, X, y, cv=10, scoring='recall'))\n",
    "    f1 = np.mean(cross_val_score(classifier, X, y, cv=10, scoring='f1'))\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate classifiers\n",
    "accuracy_bernoulli, precision_bernoulli, recall_bernoulli, f1_bernoulli = evaluate_classifier(bernoulli_nb, X, y)\n",
    "accuracy_multinomial, precision_multinomial, recall_multinomial, f1_multinomial = evaluate_classifier(multinomial_nb, X, y)\n",
    "accuracy_gaussian, precision_gaussian, recall_gaussian, f1_gaussian = evaluate_classifier(gaussian_nb, X, y)\n",
    "\n",
    "# Report results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_bernoulli}\")\n",
    "print(f\"Precision: {precision_bernoulli}\")\n",
    "print(f\"Recall: {recall_bernoulli}\")\n",
    "print(f\"F1 Score: {f1_bernoulli}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_multinomial}\")\n",
    "print(f\"Precision: {precision_multinomial}\")\n",
    "print(f\"Recall: {recall_multinomial}\")\n",
    "print(f\"F1 Score: {f1_multinomial}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_gaussian}\")\n",
    "print(f\"Precision: {precision_gaussian}\")\n",
    "print(f\"Recall: {recall_gaussian}\")\n",
    "print(f\"F1 Score: {f1_gaussian}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21a5e0-47de-4c70-90be-57ba3dbc09b1",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "The choice of the best-performing Naive Bayes variant depends on the nature of the data.\n",
    "If features are binary (presence or absence), Bernoulli Naive Bayes might perform well.\n",
    "If features represent counts or frequencies (e.g., word occurrences), Multinomial Naive Bayes could be suitable.\n",
    "Gaussian Naive Bayes assumes a Gaussian distribution of features and might work well with continuous data.\n",
    "Conclusion:\n",
    "\n",
    "Analyze the results to identify the most suitable Naive Bayes variant for the Spambase dataset.\n",
    "Consider the assumptions of each variant and whether they align with the characteristics of the data.\n",
    "Discuss any observed limitations of Naive Bayes, such as the assumption of feature independence, which may not always hold in real-world scenarios.\n",
    "Suggest future work, such as exploring more sophisticated models or feature engineering techniques.\n",
    "Remember to adapt the code an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507c35f-69cb-4141-a66c-0f7227303a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
